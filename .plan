<!-- keys- --> 
- to-do 
+ done
* dropped  


<!-- initial set-up -->
+ prepare train-data => [left, right, top_seg, top_ipm] 
+ generate dataset.json
+ fix train.py
+ train trimmed model
+ convert / visualize top_seg file in rgb
* visualize models 
* migrate to recommended pytorch / vision version
+ fix bev_segmented dataset
+ convert bev_segmented to single channel 

* use 2 classes => void + canopy

+ fix GT seg-mask resolution across files
* why are there huge black patches in GT? 
+ why is segmented-bev not centered in GT? 
* fix num_workers for train_loader
+ fix transpose=True for SegLoader
+ figure out the co-ordinate system

+ utility to generate bev frames to s3

+ visualise cropped-seg-masks-mono in rgb
+ eval on the trained model
+ generate square shaped top_seg train-data
* provision for multiple training log folders

    
<!-- fix model input -->
+ flip GT seg-mask in z-direction
+ z / y swap
+ how can y-min be negative? 
* generate 500 frames


<!-- why is loss not dropping? -->
+ scale cx, cy, fx, fy
+ review segmentation loss function calculations
+ fix loss calculations
+ fix segmentation module
+ segmentation module output should be discrete
* changing disparity channel count in costvol calculations
+ importance of transpose


<!-- implement baseline model -->
+ fix predicted mask dimensions (480 * 480)
+ transpose
+ fix SegLoader
* fix pytorch version
+ check loss function 
* [debug] check pid-output with unet output
- [debug] check with camera-intrinsics / baseline scaling
* regenerate GT data with less black / blue regions
* get rid of ground points in GT
+ check unet implementation
- toogle number of classes, 6 vs 5
- establish correspondance with eval files and dataset files
+ add segmentation mask 
- ignoring void labels
* check for degeneracy
+ model is getting stuck at local minima, need to tune learning rate
+ reduce mask size from 400 * 400 to 200 * 200
+ flip mask for z-inversion 
* introduce a priority collapsing mechanism
+ fix z vs 10-z 
+ train on smaller mask dimensions ( 5m * 5m)
* z-indices out-of-bounds error while bev_pcd -> bev_png
+ check mono --> rgb seg-mask conversion in evaluate_model.py


<!-- demo video -->
+ generate video eval script
* augment minor classes
+ check metric accuracy
+ plot stem points on a plot to check metric accuracy 
* refactoring

<!-- tune training params -->
* enable saving of intermediate epochs 
+ plot epoch loss
- decide on the the epoch count
+ increase batch size
- figure out learning rate stretegy -> [use crestereo]
- tune learning rate (refer crestereo)


<!-- enable distributed training -->
+ enable multi-gpu training
* fix logs in distributed training script
* double check distributed-training script
* what exactly happens in distributed training? 
+ why is loss different (or decreasing faster) in distributed-training?


<!-- aws based bev-generator -->
+ start aws based data-generation pipeline
+ index to track processed files
- ipm rgb 
- occlusion mask
+ add index.json

<!-- improve model baseline -->
- fix black patches
- convert black to naviagble spaces
- add visibility mask
- refactoring
- parametrize intrinsics scaling factor
- move mavis.yaml to config
- add test.yaml, train.yaml and eval.yaml

- stereo rectification 
- add augmentation for minor classes
- add hidden points mask
- passing camera orientation

- fix model image pre-processing step

- add focal loss function
- plot GT class-wise distribution
- improve loss function
- tune weighted cross entropy loss
- change z-min to 0.5 in GT-seg-mask


<!-- camera intrinsics scaling -->
- write comparison script
- establish camera intrinsics scaling params
- camera baseline scaling
- check fx vs fy scaling
- disparity scaling
- handle image resizing effect on disparity


<!-- data augmentation -->
- increase contrast for minor classes
- fill void with ground points


<!-- testing infrastructure -->
- figure out metrics
- write testing class
- figure out eval-results logging mechanism 
- segmenent input data by class
- introduce seeding mechanism
- fix eval-video.py
- fix eval-model.py
- check sbevnet utils
- EvalClass 
- integrate eval cases
- metrics ?
- train / eval / test split
- processing bev-aws to bev-model pipeline
- utility to classify dataset into [object / pole / vineyard ] heavy


<!-- test cases -->
- w/o occlusion-mask.py
- w/o stereo-rectification 
- w/o ipm rgb
- w/o ipm features
- w/o data augmentation

- plot  measure accuracy
- plot train / val / test accuracy
- run on 500 / 200 / 200
- uplaod sample dataset to git


<!-- data-handler.py -->
+ split bev-dataset into train-val-test
* rename sbevnet module to model
+ flip masks
* support for extra data-fields
+ resume training
* parametrize randomness / seeding
+ clean GT-dataset -> remove incomplete leaf folders
* generate gt data from z = 0.5 meters instead of z = 0m
+ integrate generate_json


<!-- training utils -->
- ability to resume from last epoch / checkpoint
- log every epoch 
+ plot validation error with train plot
- add test.yaml
- run model without camera intrinsics scaling
- fix train-dist logging
- integrate json path in train.yaml
- use tensorboard, weights, biases
+ plot validation loss for overfitting detection
- plot loss metrics for test-data
+ add GT to test folder
- generate validation dataset
+ integrate validation dataset in dist-train.py
- refactor train-dist.py


<!-- fix poor validation set performance -->
- check loss function weights
- check class frequency
- increase training data size
* release GPU memory on exit
* reset tensorboard canvas on exit
+ ditch tensorboard interface
- shift GT train-data in z-direction
- use ignore index in loss function
- try focal loss
- [debug] add original filename in model-dataset/seg-mask-rgb
- test RJM / gallo separately
- why are black spaces being added to the GT mask? (/home/ubuntu/stereo-occ/sbevnet/data/GT-aws/vineyards/gallo/2024_06_07_utc/svo_files/front_2024-06-04-12-40-25.svo/1196_to_1338/frame-1198/seg-mask-rgb.png)
- reason for black frame in the first frame of each section
- poles are not in straight line in GT mask (/home/ubuntu/stereo-occ/sbevnet/data/GT-aws/vineyards/RJM/2024_06_06_utc/svo_files/front_2024-06-05-09-43-13.svo/246_to_388/frame-318/seg-mask-rgb.png)
- remove degenerate frames from GT (>10% black patches)
- plot % distribution of black patches in GT
- special handling for void class in loss function
- does eval model accept x,y param? 

- handle void class through the loss function
- what could the void class encode?
- would adding occlusion mask fix the void mask problem?  
- revisit GT generation, verify camera tilt values? 
- check validation loss trend without intrinsics scaling
- tune validation loss calculations
- predictions code is wrong 
- wrong stichhing of img | img-mask

- add scripts / bash folder

- __ to _ conversion
- better comments
- add 'download bev-folder from s3' in data-handler.py  
- refactoring + better comments in data-handler.py


- add rotation matrix support to get_grid_one
- write unit test for get_grid_one
- integrate rotation matrix into train.py
- move pytest(s) to the test folder
- test original get_grid_one with pytest
- fix get_grid_one pytest
- pass batch-size to bev_cost_utils.py

- add focal-loss
- is disparity scaling (/f) correct in get_grid_one?
- clipping in get_grid_one
- increase automation level in data-handler.py
- move validation-patience to train-dist.yaml

- convert void to ground labels
- add learning rate scheduler ==> [Exponential Decay / Step Decay / Cosine Annealing /ReduceLROnPlateau] 
- Add dropout / weight decay / or other regularization to reduce overfitting
- fix evaluate-model visualization
- tune cross-weighted entropy weights
- tune focal loss
- compare validation loss for get_grid_one [1 vs 2]
- is the new get_grid_one correct?

- training on only vine stems + poles followwd by fine tuning on other classes


<!-- sbevnet  -->
- check sbevnet utils
- EvalClass 
- integrate eval cases
- metrics ?
- train / eval / test split
- processing bev-aws to bev-model pipeline
- utility to classify dataset into [object / pole / vineyard ] heavy
+ add as submodule to stereo-occ
- train one class at a time
- warm-up + cool-down 
- pytorch lightening
- pytorch fabric
- check occ-mask integration in sbevnet
- data augmentation
- undo tilt correction in sbevnet_get_grid_one
- fix / review disparity scaling in sbevnet
- fix / review / improve image pre-processing step 
- [debug] sbvenet with only [canopy + navigable-space] as output
- stereo rectification 
- handle image resizing effect on disparity
- z / y swap
- transpose effect
- review disparity scaling


+ end to end data-handling in data-handler.py
+ refactor data-handler.py
- refactor train-dist.py
- refactor sbevnet
- add scripts folder
- simplify dataloaders
+ separate model-dataset generation
- do not measure void classes for loss calculations during evaluation
- fix get_grid_one + data warping
- examine disparity scaling
- examine image pre-processing step 
- add / revisit data-augmentation step
- add regularalization 
- figure out fine-tuning strategies
- label-wise training
- option to resume training
- option to visualize results at each epoch
- add ensembling
- what does the nature of the train-validation plot say?
- possible to train + eval simultaneously / alternately?
- inspect cases in which GT has significant rotation
- what information is contained in .pth file? 



<!-- fix train-dist.py -->
+ significance of background class
- how is validation loss being calculated?
+ add seed
+ save best-train / best-val / latest.pth
+ save all intermediate epochs
- class-wise training
- co-relation betweeen train / validation loss
- fine tuning strategy
- train on single farm
- explore best practices around incorporating validation loss in training
- reduce code duplication
+ support for train on 3 gpus + eval on 1 gpu
- tune weights
- tune gamma
- introduce alpha in focalLoss
- handling cases with class imbalanace
- read training scripts

<!-- fix rotation issue -->
- make prediction for all the frames in one leaf folder 


<!-- fix evaluation.py -->
- major refactoring
- visulalize img + gt + pred in one-frame
- integrate gt in evaluate.py
- add real-time inference
- what params can be stripped off from the evaluate.yaml 
- rename to evaluate.py 
- is it good strategy to train one class at a time and then fine tune on other classes
- common training strategy in class imbalance
- add script to combine eval predictions with gt-predictions / left-image
- read evaluation scripts
- mIoU? 



- correspondance with images, gt-seg-imgs and prediction even correct is wrong
- figure out a way to save predictions / loss-curves across training runs

